dummy data:  inp = [[randint_((16,)), (0, randn(4, 384))]], token_size==8
1520: 
sos_id:8
eos_id: 9
sample_data: we prepended sos_id 8 and appended eos_id 9 with the data:
 [[tensor([8], device='cuda:0'), tensor([6, 4, 4, 3, 1, 4, 2, 0, 4, 3, 3, 6, 3, 7, 3, 3], device='cuda:0'), (0, tensor([[-1.6495,  0.4703,  1.3070,  ..., -1.2109, -0.2641,  0.5752],
        [ 0.9323, -0.2257,  1.4384,  ...,  2.8045, -1.0094, -0.1999],
        [ 0.6611,  0.2619,  0.6303,  ...,  0.0693, -1.4146,  0.7022],
        [ 0.8277,  0.7591, -0.0626,  ...,  0.6207, -1.3113, -1.1113]],
       device='cuda:0')), tensor([9], device='cuda:0')]]
 explanation of sample_data:
 [
sos_id = [tensor([8], device='cuda:0'),
input_text_tokens = tensor([6, 4, 4, 3, 1, 4, 2, 0, 4, 3, 3, 6, 3, 7, 3, 3], device='cuda:0', size 16 of value 0-7)
image_data =  (0, tensor([[-1.6495,  0.4703,  1.3070,  ..., -1.2109, -0.2641,  0.5752],
        [ 0.9323, -0.2257,  1.4384,  ...,  2.8045, -1.0094, -0.1999],
        [ 0.6611,  0.2619,  0.6303,  ...,  0.0693, -1.4146,  0.7022],
        [ 0.8277,  0.7591, -0.0626,  ...,  0.6207, -1.3113, -1.1113]],
       device='cuda:0')), size->(4*384)
    tensor([9], device='cuda:0')]]

For each "Modality" in sample_data:
1) 1st: tensor([8])
2) 2nd: tensor([6, 4, 4, 3, 1, 4, 2, 0, 4, 3, 3, 6, 3, 7, 3, 3], len--> 16
3) 3rd: 
    (tensor([[-1.6495,  0.4703,  1.3070,  ..., -1.2109, -0.2641,  0.5752],
        [ 0.9323, -0.2257,  1.4384,  ...,  2.8045, -1.0094, -0.1999],
        [ 0.6611,  0.2619,  0.6303,  ...,  0.0693, -1.4146,  0.7022],
        [ 0.8277,  0.7591, -0.0626,  ...,  0.6207, -1.3113, -1.1113]],
       device='cuda:0'))

       for only Modality: create a placeholder text tensor for modality
            modality_shape_tuple: (4,)  #shape except last dimension from (4,384)
            create a tensor of shape: math.prod(modality_shape_tuple)
            fill it with -1:  mod_temp_text_tensor([-1, -1, -1, -1],)
            meta_id, som, eom, meta: 14 10 12 67
            after processing:
            mod_temp_text_tensor2:  tensor([14, 67, 10, -1, -1, -1, -1, 12], len--> 8
            batch_modality_positions: [(0, 18, 4)]: (modality_type, offset + 1, modality_length

                1st-16th is text token, index: 0 to index: 15
                17,18,19th other meta data, index 16 to index 18,
                18+1:18+1+4 = modality token to be placed , index 19, 20, 21, 22
                offset += modality_length (4) + 2 + len(modality_meta_info) + 1 # +2 due to [som] and [eom] - then account for meta start id and modality shape information (or eventually any meta information about modality)
                offset would be 25, from 22, 23 and 24th will be meta datas
                0-24th(inc) complete text+ modality seq: index 0 to index 24
4th) tensor([9])

text: tensor([[ 8,  -->1
    6,  4,  4,  3, -- 1,  4,  2,  0, -- 4,  3,  3,  6, -- 3,  7,  3,  3,-->17
    14, 67, 10, ------->20
    -1, -1, -1, -1, ------>24
    12,  9]], device='cuda:0')--->len 26

# embed the modality tokens into latent dimesion:
creates op tensor: output = torch.zeros((batch, seq_len, dim)): 1,25,384
update -1,-1.. positions in this tensor : seq_len dimension with modality data 
output[batch_ind, offset:(offset + length)] = batch_modality_token
****
assert dim == modality_shape[1]
os latent_dim & image_last_dimension should be equal
so if image is 4,384 then latest dimension should be 384

*********************************************
num_text_tokens = 6,
dim_latent = (8),
dummy data:  inp = [[randint_((8,)), (0, randn(2, 8))]]
1520: 
sos_id:6 (num_text_tokens)
eos_id: 7 (num_text_tokens+1)
sample_data: we prepended sos_id 6 and appended eos_id 7 with the data:

explanation of sample_data:
 [
sos_id = [[tensor([6], device='cuda:0'), 
input_text_tokens = tensor([1, 3, 4, 0, 5, 2, 6, 3], device='cuda:0'), , size 8 of value 0-6)
image_data = (0, tensor([[-0.6903, -1.1202,  0.6000,  0.2336,  0.0184, -0.2423,  0.5047, -0.3649],
        [ 0.7957, -0.2994,  1.5190,  0.5542,  0.6965, -0.9146, -0.6180,  0.1649]],device='cuda:0')), 
eos_id = tensor([7], device='cuda:0')]]



For each "Modality" in sample_data:
1) 1st: tensor([6]), offset= 1
2) 2nd: tensor([1, 3, 4, 0, 5, 2, 6, 3], device='cuda:0'), len--> 8, offset=9
3) 3rd:  (0, tensor([[-0.6903, -1.1202,  0.6000,  0.2336,  0.0184, -0.2423,  0.5047, -0.3649],
        [ 0.7957, -0.2994,  1.5190,  0.5542,  0.6965, -0.9146, -0.6180,  0.1649]],device='cuda:0')), 


       for only Modality: create a placeholder text tensor for modality
            modality_shape_tuple: (2,)  #shape except last dimension from (2,8)
            create a tensor of shape: math.prod(modality_shape_tuple)
            fill it with -1:  mod_temp_text_tensor([-1, -1],)
            som, eom, meta: 8 9 61
            after processing:
            mod_temp_text_tensor2:   tensor([10, 61,  8, -1, -1,  9], len--> 6
            batch_modality_positions: [(0, 10, 2)]: (modality_type, offset + 1, modality_length)

                1st-8th is text token, index: 0 to index: 7
                9,10,11th other meta data, index 8 to index 10,
                offset += modality_length (2) + 2 + len(modality_meta_info) + 1 # +2 due to [som] and [eom] - then account for meta start id and modality shape information (or eventually any meta information about modality)
                offset would be 15, 
                0-15th(inc) complete text+ modality seq: index 0 to index 14
4th) tensor([9])

text: tensor([[ 6,  -->1
      1,  3,  4,  0,  5,  2,  6,  3,-->8
    10, 61, 8, ------->11
    -1, -1,  ------>13
    9,  7]], device='cuda:0')--->len 16

# embed the modality tokens into latent dimesion:
creates op tensor: output = torch.zeros((batch, seq_len, dim)): 1,15,8
update -1,-1.. positions in this tensor : seq_len dimension with modality data 
output[batch_ind, offset:(offset + length)] = batch_modality_token #lenght=math.prod(modality_shape_tuple)
****
assert dim == modality_shape[1]
os latent_dim & image_last_dimension should be equal
so if image is 4,384 then latent dimension should be 384

output[batch_ind, offset:(offset + length)] = batch_modality_token
offset:(offset + length) rows will be replaced with image_data_token
[
[0,0----0,0,------0]   Ncolumn= latent_dim
0
0
.
.
[-0.6903, -1.1202,  0.6000,  0.2336,  0.0184, -0.2423,  0.5047, -0.3649]
[ 0.7957, -0.2994,  1.5190,  0.5542,  0.6965, -0.9146, -0.6180,  0.1649] 
.
.
0,0----0,0,------0]
]
Nrow= seq_len

 tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000],
         [-0.6903, -1.1202,  0.6000,  0.2336,  0.0184, -0.2423,  0.5047,
          -0.3649],
         [ 0.7957, -0.2994,  1.5190,  0.5542,  0.6965, -0.9146, -0.6180,
           0.1649],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000]]],


so in the seq the placeholder (-1) positions are udpated with actual image data

modality_positions_to_is_modality_mask:
current understanding: goes through each modality types and check for their presence
int the sequence and mark those position true
one row for each modality types
type_seq: tensor([0])
mod types: tensor([[0]])
is_instance_for_type: tensor([[[True]]], device='cuda:0')
is_modality_along_seq: tensor([[[False, False, False, False, False, False, False, False, False, False,
           True,  True, False, False, False]]], device='cuda:0')
return: tensor([[[[False, False, False, False, False, False, False, False, False, False,
            True,  True, False, False, False]]]], device='cuda:0')

transformer expect input dimension: dim = 56

Now we will embedd text:
we replace all -1 with 0's: text = text.masked_fill(text == -1, 0)
text_tokens = self.text_embed(text)  #nn.Embedding(effective_num_text_tokens, xformer_dim)

output would be no_text_tokensXtransformerinput_dim: 1*15*56
15 because ( text, text_labels = text[:, :-1], text[:, 1:], 16-1=15)

text_token= 15*56

noise addition:


one md tok: 0 tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000],
         [-0.6903, -1.1202,  0.6000,  0.2336,  0.0184, -0.2423,  0.5047,
          -0.3649],
         [ 0.7957, -0.2994,  1.5190,  0.5542,  0.6965, -0.9146, -0.6180,
           0.1649],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000],
         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,
           0.0000]]], device='cuda:0') torch.Size([1, 15, 8]


ADD NOSE AND WE GET BELOW:        
modality input torch.Size([1, 15, 8]) 
1705: modality_tokens before projection: [tensor([[[ 1.7224,  0.0855, -0.8504,  0.4219, -0.4740, -0.8922, -1.1566,
          -1.1711],
         [ 0.1972,  1.7144, -0.4938,  0.7828, -0.1088, -0.0409, -2.0059,
           1.4063],
         [-0.4468, -0.5352, -0.0145,  0.7460, -2.1143, -0.6831, -0.3097,
          -0.2872],
         [ 1.2851,  0.1271,  0.5080, -0.4722, -1.6829,  0.4482, -0.2792,
           1.2965],
         [ 1.0555, -0.9246, -0.6415,  1.5163,  0.6704, -0.7244,  0.1709,
           0.4076],
         [-0.1564,  0.0493,  0.0206, -0.1181,  0.9636,  1.2800, -0.8813,
           0.4711],
         [ 0.2701,  2.3979,  0.0849,  0.7755,  0.5459, -1.7145, -1.8568,
          -0.8578],
         [-0.8999,  1.4423,  0.2806,  0.3676,  0.3936, -0.4223,  0.7511,
           0.8290],
         [ 0.3371,  0.3932, -0.8345,  0.7568, -1.5309,  0.8479, -1.5431,
          -1.4525],
         [-0.0963,  0.2241,  0.8004,  1.4738, -2.1126,  1.2572, -1.6645,
           0.2765],
         [-0.3023, -0.4349,  0.3050,  0.1948,  0.1796,  0.7251, -0.4043,
          -0.4067],
         [ 0.2202, -0.4323,  1.5167,  0.5035,  0.2481, -0.7533, -0.0776,
           0.9944],
         [-0.8503, -0.2582,  1.1757,  1.7528, -0.4331, -0.2523, -0.1580,
          -0.0249],
         [-1.6195, -0.1058, -0.7563,  0.4039,  0.4948, -1.6276,  0.1841,
           0.5189],
         [-2.2620,  1.0348, -0.5142, -1.3735, -0.7665,  0.1403, -0.5565,
          -0.2904]]], device='cuda:0')]

projection:
modality_tokens = [fn(one_modality_tokens) for fn, one_modality_tokens in zip(self.latent_to_model_projs, modality_tokens)]  #1*25*384-->25*512
modality_tokens = sum(modality_tokens)

output torch.size: 15X56

tensor([[[ 1.2981e+00, -2.4501e-01,  8.6491e-01,  2.9408e-01, -1.0180e+00,
          -7.2808e-02,  4.1097e-02, -1.9973e-01,  9.7377e-02,  2.9435e-01,
           1.2794e+00, -7.5640e-01,  3.3751e-01, -5.7829e-01,  6.7107e-01,
          -3.3159e-02, -1.3225e+00, -1.1024e+00,  5.5735e-02,  5.8517e-01,
           5.1356e-01,  5.0666e-01,  8.7350e-01, -1.0856e+00,  6.0816e-01,
           2.7288e-01, -4.7342e-01,  4.2121e-02,  4.8003e-01,  1.3031e-01,
           4.7988e-01, -1.6740e-02,  5.3186e-01,  5.9767e-01, -5.3444e-01,
          -5.8597e-01,  2.6247e-01,  2.2077e-02, -1.0366e+00, -4.3636e-03,
          -1.4832e-01,  6.6348e-01,  5.4591e-01,  5.0259e-01,  8.4344e-01,
          -1.1053e+00,  6.5398e-01,  1.3305e+00, -6.2706e-01, -1.7509e-01,
           5.1379e-01,  1.5021e-01, -1.5900e-01, -1.5266e+00, -3.1538e-01,
          -1.0621e+00],
         [ 8.5517e-01, -1.3924e-01, -2.6430e-01, -2.7921e-01, -5.1760e-01,
          -3.3394e-01,  2.8346e-01, -3.3530e-01, -9.5924e-01,  1.5968e+00,
           3.7522e-01, -1.6337e-01,  4.8712e-02,  3.3864e-01,  1.0677e-01,
           8.0990e-01, -7.3867e-01, -6.9468e-01, -4.0878e-03, -2.4969e-01,
           1.1506e+00, -3.3579e-01, -6.5744e-01, -1.8109e-02, -1.2125e-01,
          -5.7862e-01, -5.6393e-02,  2.8156e-01, -1.6931e-01,  5.6489e-01,
          -5.7131e-01,  7.5437e-01,  1.2267e-01, -2.6588e-01, -7.7445e-01,
          -8.1048e-01, -5.0634e-01,  1.1424e+00,  1.1898e+00,  8.6560e-01,
           4.6021e-02,  1.2937e+00,  6.6629e-01,  3.1125e-02,  3.1252e-01,
          -2.1927e-01,  8.9149e-01,  4.6610e-01, -1.4413e+00, -1.0594e-01,
          -1.9652e-01,  1.5311e-01,  9.8548e-02, -6.5112e-01,  2.5042e-01,
           8.6544e-02],
         [ 2.1642e-01, -4.3162e-01,  5.1714e-01, -5.9440e-01,  3.4852e-01,
          -8.3885e-02, -3.3308e-01, -1.0521e-01,  4.8207e-01, -7.7675e-01,
           7.8734e-01,  5.2103e-01,  3.9336e-01, -8.9630e-01,  1.0636e+00,
          -6.7045e-01, -2.0052e-01, -3.9988e-01,  1.8417e-01,  2.3745e-01,
           6.9819e-01,  2.6699e-01, -1.7894e-01,  1.6348e-01, -5.0879e-01,
          -3.7183e-01,  5.1964e-01, -4.9508e-01,  3.9135e-01, -6.6611e-01,
           3.2416e-02, -6.0368e-01, -5.1697e-01,  8.9174e-01, -1.7706e-01,
          -4.3362e-05, -7.0447e-01,  1.7634e-01, -7.0434e-01,  1.0793e-01,
          -7.0147e-01,  1.6278e-01,  3.4107e-01,  4.9458e-01, -4.6998e-01,
          -9.2206e-01,  4.5775e-01,  1.4597e+00,  4.0147e-01,  5.0062e-02,
          -8.4293e-01,  7.3335e-01, -4.6574e-01, -8.9763e-01, -1.0466e-01,
          -4.1180e-01],
         [-1.1672e-01, -8.3539e-01, -7.7031e-01, -5.7129e-01, -2.7104e-01,
          -8.7546e-01, -4.8103e-01,  1.6398e-01, -1.9746e-01,  1.7346e-01,
           7.4897e-01,  3.8639e-01,  6.2741e-01, -2.6814e-01,  1.2532e+00,
           6.7132e-01, -5.0003e-02, -1.0086e-02, -2.1700e-01,  7.9125e-01,
           1.0135e+00,  7.0053e-01, -4.4616e-01,  1.3765e-01, -4.4713e-01,
          -5.3499e-01,  2.6509e-01, -2.9173e-01,  4.1372e-01, -1.5896e-01,
           4.0747e-01,  4.4482e-01, -6.4097e-02,  1.5307e-01, -2.2117e-01,
           2.3550e-01, -5.7120e-02,  9.4590e-01, -3.9145e-01,  3.8085e-01,
           3.5808e-01,  1.8481e-01,  5.6088e-01,  1.6331e-01, -7.2152e-02,
          -3.1907e-01,  1.2284e+00,  1.7353e-01, -3.3295e-01,  9.6249e-01,
          -3.2552e-01,  7.9733e-01, -5.6812e-01, -2.9308e-01, -7.2438e-01,
          -3.2705e-01],
         [ 6.6333e-01, -3.2206e-01,  5.5486e-01,  3.2459e-01, -5.0666e-01,
          -3.8621e-01, -6.9256e-01, -8.4548e-01, -1.7692e-01,  1.9192e-02,
           4.6384e-01, -3.3253e-01,  1.1358e+00,  2.1738e-01,  5.1597e-01,
           1.0022e+00, -4.4685e-01,  3.0310e-02,  5.9593e-01,  1.7977e-01,
           3.8469e-01,  5.1881e-01,  6.9911e-01, -2.1500e-01,  4.4619e-02,
           1.9355e-01, -7.4041e-01, -6.6436e-01, -2.2757e-01,  6.5064e-01,
           7.5303e-01, -5.9239e-01,  4.0133e-01,  2.6498e-01, -1.3463e+00,
          -6.8853e-01,  1.1338e+00, -5.1439e-01, -7.7351e-01,  6.3735e-01,
           6.4338e-01,  1.2241e+00,  1.1223e+00,  5.2628e-01,  1.0383e+00,
          -5.3462e-02, -4.0302e-01,  1.0172e+00, -5.5753e-01,  3.3638e-01,
           7.1347e-01,  3.0517e-01,  6.0507e-01, -1.0060e+00,  4.3612e-01,
          -7.0670e-02],
         [ 2.1325e-01, -4.9598e-01, -7.0660e-01,  1.5849e-01, -3.2119e-01,
           4.7322e-03, -1.1957e-01, -8.0519e-01,  2.2710e-01,  9.1773e-01,
          -8.7980e-02, -5.1675e-03,  1.0731e-01,  6.2577e-01, -8.7061e-02,
           7.5697e-01, -7.2762e-01, -3.3583e-01,  3.6692e-01,  9.3291e-02,
           1.3331e-01, -2.8667e-01, -4.8198e-01,  4.1879e-01, -6.3342e-01,
          -9.2860e-01, -8.3455e-01,  5.8585e-01,  7.1223e-01, -1.9887e-01,
          -3.6936e-01, -2.9769e-02, -4.4093e-02, -1.0342e+00, -5.2219e-01,
          -7.4742e-01,  3.0531e-02,  2.6445e-01,  6.7054e-01,  4.5705e-01,
           1.8714e-01,  5.6053e-01, -3.2748e-01, -6.4470e-01,  5.7073e-01,
           5.9450e-01,  8.0680e-01, -1.9750e-01, -6.0828e-01, -3.7353e-01,
           1.6024e-01,  3.8518e-01,  3.1166e-01,  2.8117e-01,  4.8814e-02,
           6.0819e-01],
         [ 1.6598e+00,  4.6926e-01,  6.2185e-01, -1.6063e-01, -2.3737e-01,
           5.1706e-01,  1.0497e+00,  1.5682e-02, -8.5077e-01,  7.3510e-01,
           6.2625e-01, -7.4792e-01, -8.9500e-01, -1.0310e+00, -4.8296e-01,
           1.7366e-01, -1.2262e+00, -1.0310e+00,  1.7752e-01, -6.4022e-01,
           2.2022e-01,  2.9175e-01,  1.6011e-01, -9.9118e-01,  8.1717e-01,
           4.8291e-02,  5.3862e-01,  3.7846e-01, -3.6441e-01,  6.3857e-01,
           1.4828e-01,  5.1933e-01,  5.7204e-01,  5.0152e-01,  4.6770e-02,
          -1.0147e+00, -8.5849e-01,  1.9223e-01,  6.3730e-01,  2.2197e-01,
          -7.3850e-01,  1.2269e+00,  7.0363e-01, -3.0239e-01,  2.0836e-01,
          -7.2792e-01, -1.3140e-02,  1.1803e+00, -7.4155e-01, -1.2440e+00,
           2.1808e-01, -7.7999e-01, -8.0907e-01, -1.2018e+00,  9.7414e-01,
          -5.5330e-01],
         [ 4.7690e-01,  2.2286e-02, -7.0332e-01, -8.1170e-02,  7.0701e-01,
          -2.0601e-01, -6.9102e-03,  5.3640e-02, -7.6056e-01,  2.6531e-02,
           2.1436e-01,  3.5467e-02, -8.3188e-02, -3.7617e-01, -2.7813e-01,
           7.7349e-01,  3.5676e-01,  1.5684e-01,  3.5736e-01, -5.2782e-01,
          -2.2977e-01,  4.5238e-01, -3.9709e-01,  2.0913e-01,  9.6436e-02,
          -9.2862e-02,  6.2840e-01, -3.8806e-01, -6.3426e-01, -1.3214e-01,
          -2.5768e-02, -1.5963e-01, -3.2989e-01,  2.9876e-01, -1.8029e-01,
          -4.0840e-01, -1.9483e-01,  3.4624e-01,  5.7194e-01,  4.4137e-01,
           3.5063e-01,  5.5596e-01,  9.0147e-01, -6.4289e-01, -7.0647e-02,
           7.2351e-01, -3.8474e-01, -9.3850e-02,  8.4727e-02, -2.5708e-01,
           2.1563e-01, -3.5785e-01, -6.8709e-01,  3.1660e-01,  1.4014e+00,
           5.8333e-01],
         [ 1.1430e+00, -4.7168e-01,  4.7815e-01,  4.7419e-01, -3.1995e-01,
           2.5220e-03,  5.9651e-03, -1.6236e-01,  8.0343e-01,  6.3348e-01,
           1.7344e+00, -2.2851e-01,  3.4114e-02, -3.9141e-01,  4.9027e-01,
          -5.0916e-01, -1.5789e+00, -1.4505e+00, -1.1432e-01,  4.3673e-01,
           8.8789e-01, -1.4111e-01,  1.0218e-01, -2.1356e-01, -3.3247e-01,
          -6.4795e-01, -5.6342e-01,  4.1058e-01,  1.1415e+00, -8.4299e-01,
          -8.4033e-01,  9.3157e-02, -4.3810e-01,  4.9082e-01, -4.1641e-01,
          -7.8010e-01, -8.5171e-01,  6.1933e-01, -4.3284e-01, -1.7634e-01,
          -9.3919e-01,  1.8830e-01, -1.4318e-01,  3.5685e-01,  5.7393e-01,
          -8.0333e-01,  1.2827e+00,  1.1275e+00, -8.3526e-01, -1.9529e-01,
          -4.0738e-02,  8.4351e-01,  4.5444e-02, -9.1067e-01, -5.1248e-01,
          -7.5078e-01],
         [ 1.9988e-01, -7.4138e-01, -4.6860e-02, -4.5133e-01,  4.6136e-01,
          -2.4962e-01, -2.2414e-01, -3.3071e-01,  5.9657e-01,  4.7545e-01,
           1.0792e+00,  7.9298e-01,  1.5304e-01, -3.3516e-01,  7.6741e-01,
           1.8299e-01, -1.0132e+00, -4.5772e-01, -3.8982e-02,  1.1025e-01,
           1.6421e+00, -3.5717e-02, -8.9813e-01,  8.7140e-01, -1.4741e+00,
          -1.5606e+00,  4.5906e-02,  7.6046e-02,  8.7478e-01, -3.3773e-01,
          -6.3036e-01,  4.5677e-01, -6.2845e-01,  2.4503e-01, -4.0967e-01,
          -5.7983e-01, -1.4099e+00,  7.6055e-01,  3.0490e-01,  3.5848e-01,
          -1.2071e+00,  5.7600e-01,  4.3307e-02,  3.7527e-01, -1.3768e-01,
          -4.9761e-01,  1.1426e+00,  1.1224e+00, -9.2949e-01,  3.9863e-01,
          -9.1272e-01,  1.4232e+00,  6.4222e-02, -5.8098e-01, -6.0914e-01,
          -2.4932e-01],
         [ 2.1550e-01, -4.7727e-01, -2.0335e-01,  5.1881e-02,  8.2515e-02,
           1.7260e-01, -1.9618e-01, -6.2886e-01,  6.5095e-01,  3.2456e-02,
           2.4320e-01,  1.5734e-01,  8.8002e-02, -7.6164e-02,  1.6787e-01,
           2.0500e-01, -6.7934e-01, -3.0285e-01,  4.2227e-01,  1.3206e-01,
           3.1613e-02,  5.0626e-02, -2.0568e-01,  3.4031e-01, -6.4640e-01,
          -7.8452e-01, -4.6276e-01,  2.5785e-01,  7.7846e-01, -4.9632e-01,
          -8.1037e-02, -4.1528e-01, -2.0789e-01, -3.5717e-01, -2.5878e-01,
          -5.6212e-01, -2.0971e-01, -1.4549e-01, -3.0805e-02,  1.4211e-01,
          -2.9828e-01,  2.8901e-01, -2.7429e-01, -4.0537e-01,  2.8777e-01,
           1.8820e-01,  4.7488e-01,  4.0630e-01, -3.1865e-02, -4.0768e-01,
          -2.7398e-02,  4.9221e-01, -3.3540e-03, -2.9209e-02,  6.7991e-02,
           2.4892e-01],
         [-2.5879e-01, -3.6972e-01, -2.0172e-01, -8.8757e-01,  3.1076e-01,
          -3.9701e-02, -2.1941e-01, -5.5866e-01, -2.0218e-01, -5.3140e-01,
          -4.8863e-01,  5.1524e-01,  2.6811e-01, -4.8906e-01,  5.0288e-01,
           8.8478e-01,  7.9974e-02,  6.6258e-01,  5.6430e-01, -1.1875e-01,
           2.6544e-01,  7.1533e-01, -3.6041e-01,  3.6595e-01, -5.9302e-01,
          -5.9336e-01,  4.3058e-01, -3.9935e-01, -1.3350e-01,  5.6630e-01,
           1.0890e+00, -2.3821e-01,  3.2072e-01, -1.3377e-01, -1.7975e-01,
          -1.7807e-01,  1.8597e-02, -4.2946e-01,  4.1144e-02,  6.1179e-01,
          -1.0325e-02,  8.9159e-01,  5.5720e-01, -2.5698e-01, -2.5732e-01,
           3.7830e-02, -2.4897e-01,  6.9276e-01,  2.3035e-01, -3.4037e-02,
          -3.5728e-01,  2.2979e-01, -4.2904e-01, -3.4470e-01,  3.5575e-01,
           2.8244e-01],
         [ 1.4366e-01, -2.7499e-01,  2.7571e-01, -4.5748e-01,  9.7219e-01,
           2.0206e-01, -2.2918e-01, -5.8046e-01,  3.5876e-01, -6.0591e-01,
           2.3366e-01,  6.4814e-01,  9.8176e-02, -6.8809e-01,  2.1423e-01,
           3.4525e-01, -3.3009e-01,  2.5897e-01,  6.0759e-01, -4.7462e-01,
           4.5616e-01,  3.8535e-01, -3.9579e-01,  7.1782e-01, -9.8108e-01,
          -8.8904e-01,  3.6927e-01, -4.3272e-01,  5.9314e-02,  7.0531e-03,
           2.4166e-01, -4.7014e-01, -3.5194e-01,  4.1904e-01, -3.2206e-01,
          -6.0114e-01, -7.3124e-01, -4.5514e-01,  6.8296e-02,  3.8689e-01,
          -8.5733e-01,  8.1039e-01,  4.4766e-01, -2.5533e-02, -2.1910e-01,
           8.6892e-03, -4.4551e-01,  1.2212e+00,  7.8637e-02, -2.3678e-01,
          -4.8302e-01,  5.7914e-01, -1.5574e-01, -4.3195e-01,  6.6868e-01,
           2.3339e-01],
         [ 3.2566e-01,  1.5401e-01,  2.7653e-01, -4.5367e-01,  1.5793e-01,
           2.7218e-01, -6.6307e-02, -5.8351e-01, -3.9289e-01, -4.3663e-01,
          -5.6909e-01,  5.2517e-02,  2.4585e-01, -1.7257e-01,  8.3892e-02,
          -2.4974e-01,  4.1914e-01, -1.3144e-01,  7.1445e-01, -4.8734e-01,
          -4.0430e-01, -2.2687e-01, -1.5427e-01,  7.2104e-03,  1.8314e-01,
           2.2861e-01,  3.4313e-01, -3.8055e-01, -5.0492e-01, -1.8942e-01,
           2.0386e-02, -1.1426e+00, -1.8245e-01,  1.0886e-01, -4.5363e-01,
          -2.9099e-01,  1.0759e-01, -2.1407e-01,  2.6778e-01,  5.6300e-01,
           2.6172e-01,  6.8332e-01,  4.9141e-01, -2.2583e-01, -2.9048e-01,
          -4.5746e-02, -4.5679e-01,  7.8557e-01,  6.6361e-01, -8.6887e-01,
          -3.6583e-01, -4.0239e-01, -2.2346e-01, -3.8062e-01,  1.2332e+00,
           6.2334e-01],
         [ 2.2234e-01, -1.0185e-01, -6.6379e-01, -4.4585e-01,  3.3576e-01,
           4.2713e-01,  4.3073e-01,  8.3343e-02,  2.3265e-01,  6.3331e-02,
          -4.3998e-02,  2.4326e-01, -7.0304e-01, -4.2144e-01, -1.6213e-02,
          -1.1128e+00,  2.3850e-02, -9.3937e-01,  1.1932e-01, -1.3776e-01,
          -4.4404e-01, -6.4661e-01, -9.3339e-01,  1.8931e-01, -1.3157e-01,
          -5.0699e-01,  5.5659e-01,  6.0932e-01,  6.2020e-01, -1.5703e+00,
          -1.1239e+00, -5.3127e-01, -8.5783e-01, -2.5242e-01,  5.9404e-01,
          -4.9785e-02, -1.2076e+00,  8.6797e-01,  7.9589e-01, -6.6269e-02,
          -4.3264e-01, -5.0192e-01, -7.3994e-01, -9.4142e-01, -8.1839e-01,
          -4.0244e-02,  1.0731e+00, -5.6981e-02,  7.8506e-01, -1.2107e+00,
          -8.8774e-01, -1.4825e-01, -9.6403e-01,  4.8618e-01,  4.8029e-01,
           5.2069e-01]]], device='cuda:0', grad_fn=<AddBackward0>) torch.Size([15, 56])

text_token= 15*56
modality_tokens= 15*56

THIS LINE:   tokens = einx.where('b n, b n d, b n d', is_any_modality, modality_tokens, text_tokens) #choose from any of the text/modality in each token position
replaces text_tokens rows with modality_tokens values in rows where originaly -1 or is_modality_along_seq was True

For different data sample of xformer size 16:
text before embedd: tensor([[ 6,  4,  6,  3,  2,  2,  3,  6,  4, 10, 61,  8,  0,  0,  9]],
       device='cuda:0')
text, texttokens: torch.Size([1, 15]) torch.Size([1, 15, 16])
tensor([[[-0.1091, -0.9033, -0.2301,  0.4242,  0.7771, -0.4624,  0.4129,
           0.7123, -0.1922, -0.3099, -1.6755, -0.5079,  0.9560, -0.2613,
          -0.5137, -0.7763],
         [ 0.6092,  0.9715,  1.2560, -1.4709,  0.1124,  1.0240, -1.1268,
           0.8720,  0.4443, -0.8214, -0.4721,  0.4254,  1.8738, -0.1504,
          -0.4840,  1.2647],
         [-0.1091, -0.9033, -0.2301,  0.4242,  0.7771, -0.4624,  0.4129,
           0.7123, -0.1922, -0.3099, -1.6755, -0.5079,  0.9560, -0.2613,
          -0.5137, -0.7763],
         [-0.3856, -0.7341, -1.5038,  0.3425, -1.2237, -1.0025, -0.7061,
           1.2945, -0.5642, -0.1460, -1.2195,  0.2322, -1.0344, -1.5131,
           2.0975,  1.8121],
         [ 1.6533, -0.1419, -0.4408, -2.3949,  0.8734,  0.3162,  2.0473,
           0.2692,  1.4172, -0.9891,  1.0120, -0.4324,  1.8895, -1.6484,
           0.7475,  0.9731],
         [ 1.6533, -0.1419, -0.4408, -2.3949,  0.8734,  0.3162,  2.0473,
           0.2692,  1.4172, -0.9891,  1.0120, -0.4324,  1.8895, -1.6484,
           0.7475,  0.9731],
         [-0.3856, -0.7341, -1.5038,  0.3425, -1.2237, -1.0025, -0.7061,
           1.2945, -0.5642, -0.1460, -1.2195,  0.2322, -1.0344, -1.5131,
           2.0975,  1.8121],
         [-0.1091, -0.9033, -0.2301,  0.4242,  0.7771, -0.4624,  0.4129,
           0.7123, -0.1922, -0.3099, -1.6755, -0.5079,  0.9560, -0.2613,
          -0.5137, -0.7763],
         [ 0.6092,  0.9715,  1.2560, -1.4709,  0.1124,  1.0240, -1.1268,
           0.8720,  0.4443, -0.8214, -0.4721,  0.4254,  1.8738, -0.1504,
          -0.4840,  1.2647],
         [ 0.8772,  0.0091,  1.2588, -0.2179, -0.9955,  0.7164, -1.1312,
          -1.4098,  0.8968,  1.6594, -0.5500,  1.7695, -0.4240, -0.2037,
          -1.0377,  0.0270],
         [-0.3518, -0.2670, -0.5062,  1.8029,  0.5513,  0.7098,  0.2936,
           0.1310,  0.6628, -0.3815,  0.2161, -0.4988, -0.5500, -0.7398,
          -0.1219,  0.0891],
         [-1.2994, -0.4564,  0.3719,  1.4493,  0.7923, -1.0130, -1.5533,
          -0.4554, -0.7587, -0.6142, -0.4335, -1.3796, -0.5669,  0.0898,
          -0.3069,  0.1729],
         [-0.2010,  2.1378,  0.0687, -0.3702,  0.4032,  0.5697, -1.0114,
           0.6579,  2.4597, -0.1812, -0.0499,  0.5885, -0.1893, -0.6821,
           0.8178,  0.8323],
         [-0.2010,  2.1378,  0.0687, -0.3702,  0.4032,  0.5697, -1.0114,
           0.6579,  2.4597, -0.1812, -0.0499,  0.5885, -0.1893, -0.6821,
           0.8178,  0.8323],
         [-0.7845,  0.4061,  0.2028,  0.0376,  1.8411,  0.8436,  1.0019,
           0.6811,  0.3058,  0.4880, -0.3980,  0.7506,  0.5160,  1.1544,
           0.5574, -0.5040]]], device='cuda:0', grad_fn=<EmbeddingBackward0>)


     modality_tokens before projection: [tensor([[[ 0.0557, -1.1669, -0.7695, -0.6362, -0.6469,  0.4780,  1.8160,
          -0.8113],
         [-1.7017,  1.7951, -2.4887, -1.1138, -2.2184, -1.2876,  0.3973,
           1.1442],
         [-0.4593, -0.8404, -1.0570, -0.9586, -0.2966, -2.3208, -0.3246,
          -1.3344],
         [ 0.4719,  0.7437,  0.2209, -0.4074, -0.8925, -0.1025,  0.0800,
          -0.8831],
         [ 0.2896,  2.3942, -0.5627,  0.6163,  1.3108,  0.3031,  0.1655,
           0.5044],
         [ 0.3775,  0.7107,  1.1867,  1.1980,  0.3658,  0.2588,  0.2559,
           0.9006],
         [-0.3001, -0.4668,  0.2553, -0.3067,  0.4012,  1.0310, -0.0912,
           1.3048],
         [-0.7791,  0.6584,  0.4658, -2.4072, -1.0033,  0.2085, -1.0279,
           0.2220],
         [-1.2132, -0.0322,  1.0598, -0.9405, -0.5183,  0.2782,  0.1394,
           0.6368],
         [-1.7110, -1.1206, -1.7669,  1.0014,  0.2315,  0.5583,  0.0124,
           0.8513],
         [ 1.4833,  1.4890,  0.7491, -0.8873, -0.6798, -0.5599,  0.0832,
           0.6387],
         [-0.2922, -0.1679, -0.3424, -0.1849, -0.3605,  0.2202,  0.4327,
           0.7107],
         [-0.2142, -0.6428, -0.8808, -0.2759,  1.2190,  1.0469,  0.9071,
           1.2537],
         [-1.6445, -0.1813, -0.0867,  0.1352,  0.1451, -1.5276, -0.3035,
          -1.3784],
         [-0.6635, -0.4464,  0.5285,  0.3698,  0.0482, -1.1297, -2.0901,
           0.7786]]], device='cuda:0')] torch.Size([1, 15, 8])
1708: modality_tokens after projection: tensor([[[ 1.1805,  0.2320, -0.4348, -0.1518,  0.2574,  0.2958, -1.0464,
          -0.7648, -0.4702,  1.1433,  0.5868, -0.5495, -0.2022, -0.4625,
           0.7073,  0.2176],
         [ 1.4297,  0.2058,  0.4168,  0.9379, -0.4075, -1.3861,  0.3684,
          -0.0874, -1.5804,  0.9219,  0.3388, -0.6110, -1.8136, -0.5276,
           0.0903,  0.7121],
         [ 0.6742,  0.7094, -0.6548,  0.2979, -0.6162, -0.7699, -0.0142,
          -0.1272, -1.2766,  0.3118,  0.1355,  0.6140,  0.4909, -0.0911,
           0.3635, -1.4764],
         [ 0.4905, -0.1986,  0.2179, -0.6429,  0.6421, -0.2436, -0.1601,
           0.0593, -0.1893,  0.1419, -0.2143, -0.3316, -0.0615, -0.0483,
           0.4266, -0.1258],
         [-0.0434, -0.7964,  0.8665, -1.3501,  0.5434,  0.0541,  0.3219,
           0.9575,  0.2704, -0.8322,  0.3322, -0.3252, -1.0912,  0.8189,
           0.1509,  0.2232],
         [-0.2928, -0.1306,  0.6318, -1.0154,  0.5805, -0.0552, -0.2915,
           0.0172,  0.1702, -0.9522, -0.0463, -0.5265, -0.2754,  1.0675,
          -0.1488, -0.0640],
         [-0.1401,  0.3288, -0.0117,  0.3751, -0.0315,  0.1444,  0.0629,
          -0.1250, -0.0169, -0.0782, -0.2797,  0.0160, -0.1138,  0.2636,
          -0.3289, -0.1059],
         [ 0.2824,  0.5216,  0.0655,  0.8234,  0.1812,  0.0243,  0.9774,
           0.4467, -0.4680,  0.3121, -1.1663,  0.3927,  0.0858, -0.5809,
          -0.4552, -0.3404],
         [ 0.2999,  0.5084,  0.0020,  0.3923,  0.4380,  0.3489,  0.2304,
          -0.1019, -0.5492, -0.2165, -0.4263, -0.1501, -0.0923,  0.1067,
          -0.5989, -0.1826],
         [ 0.3753, -0.2781, -0.8766,  1.0431, -0.2228, -0.6423, -0.3368,
          -0.7494, -0.8427, -0.0381,  1.1206, -0.2550, -0.6780,  0.2765,
          -0.0625, -0.1087],
         [ 0.0843,  0.5143,  1.1604, -0.9589,  0.0547, -0.3420,  0.1305,
           0.4678,  0.1274,  0.1450, -1.0206, -0.1528, -0.1771,  0.2318,
           0.2191, -0.0311],
         [ 0.3954,  0.2553,  0.0377,  0.1523, -0.0095, -0.2394, -0.2447,
          -0.3082, -0.4074,  0.2423,  0.0736, -0.2893, -0.3951,  0.1158,
           0.0430,  0.0389],
         [ 0.2043,  0.2969, -0.0797,  0.1255, -0.3098,  0.4194, -0.2592,
          -0.1090, -0.0408,  0.2953,  0.3595, -0.0349, -0.5216,  0.2198,
           0.0056,  0.1018],
         [ 0.4833, -0.0499, -0.6985, -0.0239,  0.3918, -0.2355,  0.0942,
          -0.0189, -1.1813, -0.7948,  0.5841,  0.1611,  0.1692,  0.4588,
          -0.1352, -1.2260],
         [-0.6860,  0.4634, -0.2823,  0.7107, -0.4413, -1.1000,  0.6020,
          -0.0145, -0.6588, -1.0191, -0.5684,  0.6488,  0.6046,  0.7728,
          -0.5855, -1.4562]]], device='cuda:0', grad_fn=<AddBackward0>) torch.Size([15, 16])


row 11 and 12 of Final token matches with modalities and rest from text tokens.